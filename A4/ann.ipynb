{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "import math\n",
    "\n",
    "#Reading Train and Dev Data\n",
    "train_class_labels = []\n",
    "train_data = []\n",
    "with open(\"Synthetic_Dataset/train.txt\") as f:\n",
    "    temp = [[float(val) for val in line.strip().split(',')] for line in f]\n",
    "    for i in temp:\n",
    "        train_data.append(np.array([i[0],i[1]]))\n",
    "        train_class_labels.append(int(i[2]-1))\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "dev_class_labels = []\n",
    "dev_data = []\n",
    "with open(\"Synthetic_Dataset/dev.txt\") as f:\n",
    "    temp = [[float(val) for val in line.strip().split(',')] for line in f]\n",
    "    for i in temp:\n",
    "        dev_data.append(np.array([i[0],i[1]]))\n",
    "        dev_class_labels.append(int(i[2]-1))\n",
    "dev_data = np.array(dev_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0, hidden_layer_sizes=(10, 10), max_iter=2000,\n",
       "              random_state=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = MLPClassifier(solver='adam', alpha=0,hidden_layer_sizes=(10,10), random_state=1, max_iter = 2000)\n",
    "clf.fit(train_data, train_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "count_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == dev_class_labels[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = ['coast','forest','highway','mountain','opencountry']\n",
    "train = []  \n",
    "train_all = []\n",
    "dev_all = []\n",
    "dev = []\n",
    "test = []\n",
    "denoms = []\n",
    "priors = np.zeros(len(classes))\n",
    "train_labels = []\n",
    "dev_labels = []\n",
    "dev_img_label= []\n",
    "train_img_label= []\n",
    "\n",
    "train_imgs = []\n",
    "dev_imgs = []\n",
    "#Reading the Images of each class for train and Developement DataSets\n",
    "for i,cls in enumerate(classes):\n",
    "    lst = []\n",
    "    dir_list = os.listdir('Features//'+cls+'//train')\n",
    "    priors[i] = len(dir_list)\n",
    "    for file in dir_list:\n",
    "        lst.extend(np.loadtxt('Features/'+cls+'/train/' + file))\n",
    "        concat = np.array([])\n",
    "        for r in np.loadtxt('Features/'+cls+'/train/' + file):\n",
    "            concat = np.concatenate((concat,r))\n",
    "        train_imgs.append(concat)\n",
    "        train_labels.extend([i]*36)\n",
    "        train_img_label.extend([i])\n",
    "    lst = np.array(lst)\n",
    "    train.append(lst)\n",
    "    train_all.extend(lst)\n",
    "\n",
    "    lst = []\n",
    "    lst2 = []\n",
    "    dir_list = os.listdir('Features//'+cls+'//dev')\n",
    "    for file in dir_list:\n",
    "        lst.append(np.loadtxt('Features/'+cls+'/dev/' + file))\n",
    "        lst2.extend(np.loadtxt('Features/'+cls+'/dev/' + file))\n",
    "        concat = np.array([])\n",
    "        for r in np.loadtxt('Features/'+cls+'/dev/' + file):\n",
    "            concat = np.concatenate((concat,r))\n",
    "        dev_imgs.append(concat)\n",
    "        dev_labels.extend([i]*36)\n",
    "        dev_img_label.extend([i])\n",
    "    dev.append(np.array(lst))\n",
    "    test.append(np.array(lst2))\n",
    "    dev_all.extend(lst2)\n",
    "# train_all = np.array(train_all)\n",
    "# dev_all = np.array(dev_all)\n",
    "#Mean Normalisation\n",
    "mean_train = np.mean(train_all,axis=0)\n",
    "maxs = np.max(train_all,axis=0)\n",
    "mins = np.min(train_all,axis=0)\n",
    "denoms = maxs - mins\n",
    "train_all = (train_all-mean_train)/denoms\n",
    "dev_all = (dev_all-mean_train)/denoms\n",
    "\n",
    "\n",
    "\n",
    "#Principal Component Analysis, for reducing Dimensionality\n",
    "pca = PCA(.99)\n",
    "pca.fit(np.array(train_all))\n",
    "train_all = pca.transform(train_all)\n",
    "dev_all = pca.transform(dev_all)\n",
    "\n",
    "train_all = np.array(train_all)\n",
    "dev_all = np.array(dev_all)\n",
    "\n",
    "#Principal Component Analysis, for reducing Dimensionality\n",
    "pca = PCA(.9)\n",
    "pca.fit(np.array(train_imgs))\n",
    "train_imgs = pca.transform(train_imgs)\n",
    "dev_imgs = pca.transform(dev_imgs)\n",
    "\n",
    "train_imgs = np.array(train_imgs)\n",
    "dev_imgs = np.array(dev_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1210, 136)\n"
     ]
    }
   ],
   "source": [
    "print(train_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, hidden_layer_sizes=(40, 20, 10),\n",
       "              max_iter=2000, random_state=1, solver='sgd')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='sgd', activation=\"tanh\",alpha=0,hidden_layer_sizes=(40,20,10), random_state=1, max_iter = 2000)\n",
    "clf.fit(train_all, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(dev_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 4 0 4 3 0 0 2 0 3 4 0 0 0 0 0 0 0 2 0 1\n",
      " 0 0 0 0 0 2 0 0 0 0 3 0 3 3 0 0 0 0 0 2 0 0 0 0 2 2 0 3 0 3 0 3 0 0 3 0 1\n",
      " 4 1 4 3 4 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 3 3 1 3 3 1 1 1 1 3 1 1\n",
      " 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 1 1 1 4 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 4 0 4 2 2 2 2 2 2 2 2 2 2 2 3 0 3 3 2 3 0 3 3 3 3 3 3 3 3 3 4 3 2 3 2 4 3\n",
      " 0 3 0 0 0 4 3 3 3 3 0 3 1 3 3 1 3 3 3 2 0 3 3 2 3 1 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 4 3 3 1 1 3 1 3 1 3 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4\n",
      " 3 3 3 1 3 3 3 0 3 1 1 4 4 1 4 1 1 1 1 0 4 0 3 3 4 1 4 4 3 3 3 1 4 1 1 1 3\n",
      " 3 1 4 4 1 3 1 4 1 0 0 1 1 4 3 4 4 4 4 3 3 0 3 4 1 1 4 3 0 3 4 1 1 2 1 1 1\n",
      " 1 1 4 1 1 4 4 1 3 1 0 3 1 4 0]\n"
     ]
    }
   ],
   "source": [
    "predictions_img = []\n",
    "for i in range(len(dev_img_label)):\n",
    "    predictions_img.append(np.bincount(predictions[36*i:36*(i+1)]).argmax())\n",
    "predictions_img = np.array(predictions_img)\n",
    "print(predictions_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4492337164750958\n"
     ]
    }
   ],
   "source": [
    "count_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == dev_labels[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(dev_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5919540229885057\n"
     ]
    }
   ],
   "source": [
    "count_correct = 0\n",
    "for i in range(len(predictions_img)):\n",
    "    if predictions_img[i] == dev_img_label[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(dev_img_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5517676767676768\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train_all)\n",
    "\n",
    "count_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == train_labels[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(train_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 3 4 0]\n",
      "0.7702479338842976\n"
     ]
    }
   ],
   "source": [
    "predictions_img = []\n",
    "for i in range(len(train_img_label)):\n",
    "    predictions_img.append(np.bincount(predictions[36*i:36*(i+1)]).argmax())\n",
    "predictions_img = np.array(predictions_img)\n",
    "print(predictions_img)\n",
    "\n",
    "count_correct = 0\n",
    "for i in range(len(predictions_img)):\n",
    "    if predictions_img[i] == train_img_label[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(train_img_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN with 828 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=1, hidden_layer_sizes=50, max_iter=5000,\n",
       "              random_state=1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='adam', activation=\"tanh\",alpha=1,hidden_layer_sizes=(50), random_state=1, max_iter = 5000)\n",
    "clf.fit(train_imgs, train_img_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9239669421487603\n"
     ]
    }
   ],
   "source": [
    "count_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == train_img_label[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(train_img_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6293103448275862\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(dev_imgs)\n",
    "count_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == dev_img_label[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(dev_img_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN ISOLATED DIGITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolated Digits\n",
    "digits = [1,2,5,9,'z']\n",
    "train = []\n",
    "dev = []\n",
    "\n",
    "#Loading train and dev data\n",
    "for digit in digits:\n",
    "    lst = []\n",
    "    dir_list = os.listdir('Isolated_Digits//'+str(digit)+'//train')\n",
    "    for file in dir_list:\n",
    "        if file.endswith('.mfcc'):\n",
    "            lst.append(np.loadtxt('Isolated_Digits/'+str(digit)+'/train/' + file,skiprows = 1))\n",
    "\n",
    "    train.append(np.array(lst,dtype=object))\n",
    "\n",
    "    lst = []\n",
    "    dir_list = os.listdir('Isolated_Digits//'+str(digit)+'//dev')\n",
    "    for file in dir_list:\n",
    "        if file.endswith('.mfcc'):\n",
    "            lst.append(np.loadtxt('Isolated_Digits/'+str(digit)+'/dev/' + file,skiprows = 1))\n",
    "\n",
    "    dev.append(np.array(lst,dtype=object))\n",
    "\n",
    "count = 0\n",
    "num_frames = 0\n",
    "for cls in range(len(train)):\n",
    "    for i in range(len(train[cls])):\n",
    "        num_frames += len(train[cls][i])\n",
    "        count += 1\n",
    "avg_num_frames = math.floor(num_frames/count)\n",
    "\n",
    "train_all = []\n",
    "dev_all = []\n",
    "train_labels = []\n",
    "dev_labels = []\n",
    "for cls in range(len(train)):\n",
    "    for i in range(len(train[cls])):\n",
    "        train_all.append(signal.resample(train[cls][i],avg_num_frames))\n",
    "        train_labels.append(cls)\n",
    "\n",
    "for cls in range(len(dev)):\n",
    "    for i in range(len(dev[cls])):\n",
    "        dev_all.append(signal.resample(dev[cls][i],avg_num_frames))\n",
    "        dev_labels.append(cls)\n",
    "\n",
    "train_all = np.array(train_all)\n",
    "dev_all = np.array(dev_all)\n",
    "\n",
    "mean_train = np.mean(train_all,axis=0)\n",
    "maxs = np.max(train_all,axis=0)\n",
    "mins = np.min(train_all,axis=0)\n",
    "denoms = maxs - mins\n",
    "train_all = (train_all-mean_train)/denoms\n",
    "dev_all = (dev_all-mean_train)/denoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extended = []\n",
    "dev_extended = []\n",
    "for i in range(len(train_all)):\n",
    "    lst = []\n",
    "    for j in range(len(train_all[i])):\n",
    "        lst.extend(train_all[i][j])\n",
    "    train_extended.append(np.array(lst))\n",
    "\n",
    "for i in range(len(dev_all)):\n",
    "    lst = []\n",
    "    for j in range(len(dev_all[i])):\n",
    "        lst.extend(dev_all[i][j])\n",
    "    dev_extended.append(np.array(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.1, hidden_layer_sizes=(30, 30),\n",
       "              max_iter=5000, random_state=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='adam', activation=\"logistic\",alpha=0.1,hidden_layer_sizes=(30,30), random_state=1, max_iter = 5000)\n",
    "clf.fit(train_extended, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(dev_extended)\n",
    "count_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == dev_labels[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN telugu charr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handwriting Data\n",
    "letters = ['a','bA','chA','lA','tA']\n",
    "train = []\n",
    "dev = []\n",
    "\n",
    "#Loading train and dev data\n",
    "for letter in letters:\n",
    "    lst = []\n",
    "    dir_list = os.listdir('Handwriting_Data//'+letter+'//train')\n",
    "    for file in dir_list:\n",
    "        temp = np.loadtxt('Handwriting_Data/'+letter+'/train/' + file)[1:]\n",
    "        train_lst = np.array([temp[::2],temp[1::2]]).T\n",
    "        train_lst = train_lst.tolist()\n",
    "        #Here we add an extra feature as mentioned in the report.This feature is arctan(slope) which is basically the angle made by tangent with x axis\n",
    "        #For reason we add this angle in radians and not the slope directly, please refer to the report\n",
    "        for i in range(len(train_lst)-1): \n",
    "            if train_lst[i+1][0] != train_lst[i][0]: \n",
    "                train_lst[i].append(np.arctan((train_lst[i+1][1] - train_lst[i][1])/(train_lst[i+1][0] - train_lst[i][0])))\n",
    "            else: #If x coordinates of consecutive points are same, then we cannot divide by x2-x1 and hence directly take arctan on +-np.inf\n",
    "                if train_lst[i+1][1] - train_lst[i][1] > 0: #arctan(np.inf) if y2 - y1 > 0\n",
    "                    train_lst[i].append(np.arctan(np.inf))\n",
    "                else:\n",
    "                    train_lst[i].append(np.arctan(-np.inf)) #arctan(-np.inf) if y1 - y2 > 0\n",
    "        train_lst[len(train_lst) - 1].append(train_lst[len(train_lst) - 2][2])\n",
    "\n",
    "        lst.append(np.array(train_lst))\n",
    "\n",
    "    train.append(lst)\n",
    "\n",
    "    lst = []\n",
    "    dir_list = os.listdir('Handwriting_Data//'+letter+'//dev')\n",
    "    for file in dir_list:\n",
    "        temp = np.loadtxt('Handwriting_Data/'+letter+'/dev/' + file)[1:]\n",
    "        test_lst = np.array([temp[::2],temp[1::2]]).T\n",
    "        test_lst = test_lst.tolist()\n",
    "        for i in range(len(test_lst)-1):# The same extra feature of angle is added to the dev data too\n",
    "            if test_lst[i+1][0] != test_lst[i][0]: \n",
    "                test_lst[i].append(np.arctan((test_lst[i+1][1] - test_lst[i][1])/(test_lst[i+1][0] - test_lst[i][0])))\n",
    "            else: #If x coordinates of consecutive points are same, then we cannot divide by x2-x1 and hence directly take arctan on +-np.inf\n",
    "                if test_lst[i+1][1] - test_lst[i][1] > 0: #arctan(np.inf) if y2 - y1 > 0\n",
    "                    test_lst[i].append(np.arctan(np.inf))\n",
    "                else:\n",
    "                    test_lst[i].append(np.arctan(-np.inf)) #arctan(-np.inf) if y1 - y2 > 0\n",
    "        test_lst[len(test_lst) - 1].append(test_lst[len(test_lst) - 2][2])\n",
    "\n",
    "        lst.append(np.array(test_lst))\n",
    "\n",
    "    dev.append(lst)\n",
    "\n",
    "count = 0\n",
    "num_frames = 0\n",
    "for cls in range(len(train)):\n",
    "    for i in range(len(train[cls])):\n",
    "        num_frames += len(train[cls][i])\n",
    "        count += 1\n",
    "avg_num_frames = math.floor(num_frames/count)\n",
    "\n",
    "train_all = []\n",
    "dev_all = []\n",
    "train_labels = []\n",
    "dev_labels = []\n",
    "for cls in range(len(train)):\n",
    "    for i in range(len(train[cls])):\n",
    "        train_all.append(signal.resample(train[cls][i],avg_num_frames))\n",
    "        train_labels.append(cls)\n",
    "\n",
    "for cls in range(len(dev)):\n",
    "    for i in range(len(dev[cls])):\n",
    "        dev_all.append(signal.resample(dev[cls][i],avg_num_frames))\n",
    "        dev_labels.append(cls)\n",
    "\n",
    "train_all = np.array(train_all)\n",
    "dev_all = np.array(dev_all)\n",
    "\n",
    "mean_train = np.mean(train_all,axis=0)\n",
    "maxs = np.max(train_all,axis=0)\n",
    "mins = np.min(train_all,axis=0)\n",
    "denoms = maxs - mins\n",
    "train_all = (train_all-mean_train)/denoms\n",
    "dev_all = (dev_all-mean_train)/denoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extended = []\n",
    "dev_extended = []\n",
    "for i in range(len(train_all)):\n",
    "    lst = []\n",
    "    for j in range(len(train_all[i])):\n",
    "        lst.extend(train_all[i][j])\n",
    "    train_extended.append(np.array(lst))\n",
    "\n",
    "for i in range(len(dev_all)):\n",
    "    lst = []\n",
    "    for j in range(len(dev_all[i])):\n",
    "        lst.extend(dev_all[i][j])\n",
    "    dev_extended.append(np.array(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=2, hidden_layer_sizes=(100, 50),\n",
       "              max_iter=5000, random_state=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='adam', activation=\"tanh\",alpha=2,hidden_layer_sizes=(100,50), random_state=1, max_iter = 5000)\n",
    "clf.fit(train_extended, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(dev_extended)\n",
    "count_correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == dev_labels[i]:\n",
    "        count_correct += 1\n",
    "print(count_correct/len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db4b5fba534949474536e70b49beb95b054288be496444c82107b260edb1c537"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
